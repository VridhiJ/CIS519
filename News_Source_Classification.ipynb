{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ftfy huggingface_hub scikit-learn transformers datasets optuna accelerate==0.27.2 --quiet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import ftfy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.foxnews.com/lifestyle/jack-carrs-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.foxnews.com/entertainment/bruce-wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.foxnews.com/politics/blinken-meets...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.foxnews.com/entertainment/emily-bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.foxnews.com/media/the-view-co-host...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url\n",
       "0  https://www.foxnews.com/lifestyle/jack-carrs-e...\n",
       "1  https://www.foxnews.com/entertainment/bruce-wi...\n",
       "2  https://www.foxnews.com/politics/blinken-meets...\n",
       "3  https://www.foxnews.com/entertainment/emily-bl...\n",
       "4  https://www.foxnews.com/media/the-view-co-host..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/VridhiJ/CIS519/refs/heads/main/Dataset/news_urls.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Display the first few rows to verify the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3805 entries, 0 to 3804\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   url     3805 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 29.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Headline Collection Method\n",
    "\n",
    "Collect the news headlines by scraping multiple news websites using BeautifulSoup libraries. The scraping process involved:\n",
    "\n",
    "1. Fetching Webpages:\n",
    "\n",
    "  - Sending HTTP requests to news article URLs.\n",
    "\n",
    "  - Using appropriate headers to mimic a real browser and avoid blocking.\n",
    "    - User-Agent: Identifies the client making request. Helps avoid bot detection by mimicking real browser behavior.\n",
    "    - Accept-Charset:  Specifies the character encodings that the client can process. Helps ensure proper text rendering.\n",
    "    - Accept: Defines the type of content the client expects from the server.\n",
    "    - Accept-Language: Specifies the preferred language for the response content. Helps receive content in a readable format when a website supports multiple languages.\n",
    "    - referer: Indicates the URL of the page that made the request.\n",
    "    \n",
    "\n",
    "2. Extracting Headlines:\n",
    "\n",
    "  - Parsing the webpage content with BeautifulSoup.\n",
    "\n",
    "  - Identifying and extracting headlines using H1 tags and class attributes related to headlines.\n",
    "\n",
    "  - Handling variations in website structures dynamically.\n",
    "\n",
    "3. Error Handling & Optimization:\n",
    "\n",
    "  - Implementing error handling to skip unavailable pages.\n",
    "\n",
    "4. Storing Data:\n",
    "\n",
    "  - Storing extracted headlines in a structured pandas DataFrame.\n",
    "\n",
    " - Saving the data in CSV format for further processing.\n",
    "\n",
    "This method ensures efficient and scalable data collection while minimizing disruptions caused by website restrictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Scraping (don't rerun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get headline from a single URL\n",
    "def get_article_headline(url):\n",
    "  try:\n",
    "    user_agents = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',\n",
    "    ]\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    headers = {\n",
    "    'user-agent': random.choice(user_agents),\n",
    "    \"Accept-Charset\": \"utf-8\",\n",
    "    \"Accept\": \"*/*\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"referer\": \"https://www.google.com/\",\n",
    "    }\n",
    "    time.sleep(2)\n",
    "\n",
    "    response = requests.get(url, headers = headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "      print(f\"Warning: Failed to load page {url} (Status Code: {response.status_code})\")\n",
    "      return None  # Don't stop execution, just return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # To find headline of various types of classes\n",
    "    headline = soup.find(\"h1\", class_=lambda c: c and \"headline\" in c)\n",
    "\n",
    "    if headline:\n",
    "      headline = ftfy.fix_text(headline.get_text())  # Fix any encoding issues\n",
    "      return headline.strip()  # Return the cleaned headline\n",
    "    else:\n",
    "      return None  # Return None if no headline is found\n",
    "  except Exception as e:\n",
    "    print(f\"Error processing {url}: {e}\")\n",
    "    return None  # Return None in case of an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to load page https://www.nbcnews.com/feature/nbc-out/lil-nas-x-dolly-parton-lena-waithe-appear-virtual-glaad-n1233424 (Status Code: 404)\n",
      "Warning: Failed to load page https://www.nbcnews.com/select/shopping/what-we-bought-black-friday-cyber-monday-rcna126998 (Status Code: 500)\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list to store the headlines\n",
    "headlines = []\n",
    "\n",
    "# Loop through the URLs in your dataframe\n",
    "for url in df['url']:\n",
    "    headline = get_article_headline(url)\n",
    "    headlines.append(headline)\n",
    "\n",
    "# Add the scraped headlines to your dataframe\n",
    "df['headline'] = headlines\n",
    "\n",
    "# Show the first few rows with the scraped headlines\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"scraped_headlines.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import create_repo\n",
    "\n",
    "# Create a repository on Hugging Face Hub\n",
    "repo_name = 'scraped-headlines'\n",
    "create_repo(repo_name, private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import upload_file\n",
    "\n",
    "upload_file(\n",
    "    path_or_fileobj='scraped_headlines.csv',\n",
    "    path_in_repo='scraped_headlines_v4.csv',\n",
    "    repo_id= 'VridhiJain/scraped-headlines'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d14d11ca6c462c921066f3f5866b93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()  # enter your Hugging Face token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.foxnews.com/lifestyle/jack-carrs-e...</td>\n",
       "      <td>Jack Carr recalls Gen. Eisenhower's D-Day memo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.foxnews.com/entertainment/bruce-wi...</td>\n",
       "      <td>Bruce Willis, Demi Moore avoided doing one thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.foxnews.com/politics/blinken-meets...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.foxnews.com/entertainment/emily-bl...</td>\n",
       "      <td>Emily Blunt says her 'toes curl' when people t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.foxnews.com/media/the-view-co-host...</td>\n",
       "      <td>'The View' co-host, CNN commentator Ana Navarr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://www.foxnews.com/lifestyle/jack-carrs-e...   \n",
       "1  https://www.foxnews.com/entertainment/bruce-wi...   \n",
       "2  https://www.foxnews.com/politics/blinken-meets...   \n",
       "3  https://www.foxnews.com/entertainment/emily-bl...   \n",
       "4  https://www.foxnews.com/media/the-view-co-host...   \n",
       "\n",
       "                                            headline  \n",
       "0  Jack Carr recalls Gen. Eisenhower's D-Day memo...  \n",
       "1  Bruce Willis, Demi Moore avoided doing one thi...  \n",
       "2                                                NaN  \n",
       "3  Emily Blunt says her 'toes curl' when people t...  \n",
       "4  'The View' co-host, CNN commentator Ana Navarr...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "repo_id = \"VridhiJain/scraped-headlines\"  # repo name\n",
    "filename = \"scraped_headlines_v4.csv\"  # file name\n",
    "\n",
    "# Download the file\n",
    "file_path = hf_hub_download(repo_id=repo_id, filename=filename)\n",
    "\n",
    "# Load into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3805 entries, 0 to 3804\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   url       3805 non-null   object\n",
      " 1   headline  3352 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 59.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url           0\n",
      "headline    453\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in the dataset\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Drop any rows where the headline is missing/duplicates\n",
    "df = df.dropna(subset=['headline']).drop_duplicates(subset=['headline'])\n",
    "\n",
    "# Reset index after dropping rows\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3336 entries, 0 to 3335\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   url       3336 non-null   object\n",
      " 1   headline  3336 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 52.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "url\n",
       "False    1779\n",
       "True     1557\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['url'].str.contains('foxnews').value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fox News Headlines: 1779\n",
    "\n",
    "NBC News Headlines: 1557"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model(TF-IDF + Log Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if using GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7036\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.65      0.69       335\n",
      "           1       0.68      0.76      0.72       333\n",
      "\n",
      "    accuracy                           0.70       668\n",
      "   macro avg       0.71      0.70      0.70       668\n",
      "weighted avg       0.71      0.70      0.70       668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the preprocessed headline data from Hugging Face\n",
    "from huggingface_hub import hf_hub_download\n",
    "csv_path = hf_hub_download(repo_id=\"VridhiJain/scraped-headlines\", filename=\"scraped_headlines_v4.csv\")\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Drop rows with missing headlines\n",
    "df = df.dropna(subset=['headline']).drop_duplicates(subset=['headline'])\n",
    "\n",
    "# Label: 1 for FoxNews, 0 for NBC\n",
    "df['label'] = df['url'].apply(lambda x: 1 if \"foxnews\" in x else 0)\n",
    "\n",
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['headline'], df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=100)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Train Logistic Regression\n",
    "baseline_model = LogisticRegression(max_iter=100)\n",
    "baseline_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = baseline_model.predict(X_test_tfidf)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert-based Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from datasets import Dataset\n",
    "from transformers import (AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac4aed3baa6c4d06923ec7fd2f51772d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf717d8b63464f30995c410f5f90880e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "453e0b0a550f440897fb00e104098fbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a4b779905cb49a3b2e6ed5ef1ce3543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "689f20d5eea94cba9b2b4660dbf3c915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c12817fe0e124abeb0ef5ac57b87c342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e364ad27c7354e9185318ffba5bdd743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='501' max='501' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [501/501 03:47, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.455493</td>\n",
       "      <td>0.791916</td>\n",
       "      <td>0.778309</td>\n",
       "      <td>0.774603</td>\n",
       "      <td>0.782051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.404934</td>\n",
       "      <td>0.832335</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.828947</td>\n",
       "      <td>0.807692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.373600</td>\n",
       "      <td>0.438885</td>\n",
       "      <td>0.821856</td>\n",
       "      <td>0.793043</td>\n",
       "      <td>0.866920</td>\n",
       "      <td>0.730769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='42' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [42/42 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Evaluation Results:\n",
      "{'eval_loss': 0.43888476490974426, 'eval_accuracy': 0.8218562874251497, 'eval_f1': 0.7930434782608695, 'eval_precision': 0.8669201520912547, 'eval_recall': 0.7307692307692307, 'eval_runtime': 5.7994, 'eval_samples_per_second': 115.184, 'eval_steps_per_second': 7.242, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "df['label'] = df['url'].apply(lambda x: 1 if \"foxnews\" in x.lower() else 0)\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df[['headline', 'label']])\n",
    "test_dataset = Dataset.from_pandas(test_df[['headline', 'label']])\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"headline\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# Load model\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Define evaluation metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = torch.argmax(torch.tensor(pred.predictions), axis=1).numpy()\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return{\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=bert_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "results = trainer.evaluate()\n",
    "print(\"BERT Evaluation Results:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/VridhiJain/bert_vanilla_2', endpoint='https://huggingface.co', repo_type='model', repo_id='VridhiJain/bert_vanilla_2')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "# Repo name\n",
    "repo_name = \"bert_vanilla_2\"\n",
    "\n",
    "# This creates a repo under your namespace (username)\n",
    "api.create_repo(repo_id=repo_name, private=False, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9ceb8b0c9934b35a26646ca366d340b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login, login\n",
    "\n",
    "notebook_login()  # enter your Hugging Face token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb8674559e6741319746d8ca3a220cfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8ef9932cade4644a10c315ee2105dd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/VridhiJain/bert_vanilla_2/commit/e6f7f5925674cb968f47a819960aaedfd7a73511', commit_message='Upload tokenizer', commit_description='', oid='e6f7f5925674cb968f47a819960aaedfd7a73511', pr_url=None, repo_url=RepoUrl('https://huggingface.co/VridhiJain/bert_vanilla_2', endpoint='https://huggingface.co', repo_type='model', repo_id='VridhiJain/bert_vanilla_2'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push model and tokenizer\n",
    "trainer.model.push_to_hub(\"bert_vanilla_2\")\n",
    "tokenizer.push_to_hub(\"bert_vanilla_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa-based Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5fab6596841449298eb20235628d506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f36e15adf1734db6befee41c20fdefec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d6484d2fee145ea9d844100d65dcf85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='410' max='501' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [410/501 03:03 < 00:40, 2.22 it/s, Epoch 2.45/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.564700</td>\n",
       "      <td>0.428925</td>\n",
       "      <td>0.806886</td>\n",
       "      <td>0.774869</td>\n",
       "      <td>0.850575</td>\n",
       "      <td>0.711538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.351700</td>\n",
       "      <td>0.384277</td>\n",
       "      <td>0.850299</td>\n",
       "      <td>0.844720</td>\n",
       "      <td>0.819277</td>\n",
       "      <td>0.871795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "tokenizer_roberta = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "train_dataset = Dataset.from_pandas(train_df[[\"headline\", \"label\"]])\n",
    "test_dataset = Dataset.from_pandas(test_df[[\"headline\", \"label\"]])\n",
    "\n",
    "def tokenize_roberta(batch):\n",
    "    return tokenizer_roberta(batch[\"headline\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_roberta, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_roberta, batched=True)\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "roberta_model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "\n",
    "training_args_roberta = TrainingArguments(\n",
    "    output_dir=\"./roberta_results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer_roberta = Trainer(\n",
    "    model=roberta_model,\n",
    "    args=training_args_roberta,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer_roberta.train()\n",
    "roberta_results = trainer_roberta.evaluate()\n",
    "print(\"RoBERTa Evaluation Results:\", roberta_results)\n",
    "\n",
    "trainer_roberta.save_model(\"./roberta_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/VridhiJain/roberta_vanilla_2', endpoint='https://huggingface.co', repo_type='model', repo_id='VridhiJain/roberta_vanilla_2')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Repo name\n",
    "repo_name = \"roberta_vanilla_2\"\n",
    "\n",
    "# This creates a repo under your namespace (username)\n",
    "api.create_repo(repo_id=repo_name, private=False, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d503ab1deda45a1bec59ce71519903e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "755a33b635784c0cb6ed59306288a865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/VridhiJain/roberta_vanilla_2/commit/9363dd0596fd13ff974bc4ae19eb893809bb37d6', commit_message='Upload tokenizer', commit_description='', oid='9363dd0596fd13ff974bc4ae19eb893809bb37d6', pr_url=None, repo_url=RepoUrl('https://huggingface.co/VridhiJain/roberta_vanilla_2', endpoint='https://huggingface.co', repo_type='model', repo_id='VridhiJain/roberta_vanilla_2'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push model and tokenizer\n",
    "trainer.model.push_to_hub(\"roberta_vanilla_2\")\n",
    "tokenizer.push_to_hub(\"roberta_vanilla_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert-based Classifier - Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f16c3653bc9944a5872e098311ddb550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2648 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea4e52d76014a56b5950f376fc5d177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/663 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df['label'] = df['url'].apply(lambda x: 1 if \"foxnews\" in x.lower() else 0)\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df[['headline', 'label']])\n",
    "test_dataset = Dataset.from_pandas(test_df[['headline', 'label']])\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"headline\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "  labels = pred.label_ids\n",
    "  preds = torch.argmax(torch.tensor(pred.predictions), axis=1).numpy()\n",
    "  precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "  acc = accuracy_score(labels, preds)\n",
    "  return{\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (AutoModelForSequenceClassification, Trainer, TrainingArguments)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# define search space\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import (AutoModelForSequenceClassification, Trainer, TrainingArguments)\n",
    "import numpy as np\n",
    "\n",
    "# define search space\n",
    "learning_rates = [1e-5, 2e-5, 3e-5, 5e-5]\n",
    "epochs = [3, 4, 5]\n",
    "weight_decays = [0.01, 0.001]\n",
    "\n",
    "best_f1 = 0\n",
    "best_config = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "  for num_epochs in epochs:\n",
    "    for wd in weight_decays:\n",
    "      print(f\"\\nTraining with lr={lr}, epochs={num_epochs}, weight_decay={wd}\")\n",
    "\n",
    "      training_args = TrainingArguments(\n",
    "        output_dir=f\"./bert_tuned_lr{lr}_ep{num_epochs}_wd{wd}\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=num_epochs,\n",
    "        weight_decay=wd,\n",
    "        report_to=\"none\"\n",
    "      )\n",
    "\n",
    "      model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "      trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        compute_metrics=compute_metrics\n",
    "      )\n",
    "\n",
    "      trainer.train()\n",
    "      eval_results = trainer.evaluate()\n",
    "      print(\"F1 score:\", eval_results['eval_f1'])\n",
    "\n",
    "      if eval_results['eval_f1'] > best_f1:\n",
    "        best_f1 = eval_results['eval_f1']\n",
    "        best_config = {\n",
    "            \"learning_rate\": lr,\n",
    "            \"num_epochs\": num_epochs,\n",
    "            \"weight_decay\": wd,\n",
    "            \"eval_results\": eval_results\n",
    "        }\n",
    "\n",
    "print(\"\\nBest configuration:\")\n",
    "print(best_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert-based Classifier retrained w/ best hyperparameters (from grid search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best configuration (best F1):\n",
    "- Learning rate: 5e-05 \n",
    "- Num epochs: 5 \n",
    "- Weight decay: 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Optimization (using optuna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "def model_init():\n",
    "  return AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "def objective(trial):\n",
    "  # hyperparameter search space\n",
    "  learning_rate = trial.suggest_float(\"learning_rate\", 1e-6, 5e-5, log=True)\n",
    "  weight_decay = trial.suggest_float(\"weight_decay\", 0.0, 0.3)\n",
    "  batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32])\n",
    "  num_train_epochs = trial.suggest_int(\"num_train_epochs\", 2, 5)\n",
    "\n",
    "  args = TrainingArguments(\n",
    "    output_dir=f\"./bert_bayesian_tuned_lr{learning_rate}_ep{num_train_epochs}_wd{weight_decay}\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    report_to=\"none\"\n",
    "  )\n",
    "\n",
    "  trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "  )\n",
    "\n",
    "  trainer.train()\n",
    "  eval_result = trainer.evaluate()\n",
    "  return eval_result[\"eval_f1\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run optimization loop\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
