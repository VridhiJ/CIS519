{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ftfy huggingface_hub scikit-learn transformers datasets optuna --quiet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import ftfy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.foxnews.com/lifestyle/jack-carrs-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.foxnews.com/entertainment/bruce-wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.foxnews.com/politics/blinken-meets...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.foxnews.com/entertainment/emily-bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.foxnews.com/media/the-view-co-host...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url\n",
       "0  https://www.foxnews.com/lifestyle/jack-carrs-e...\n",
       "1  https://www.foxnews.com/entertainment/bruce-wi...\n",
       "2  https://www.foxnews.com/politics/blinken-meets...\n",
       "3  https://www.foxnews.com/entertainment/emily-bl...\n",
       "4  https://www.foxnews.com/media/the-view-co-host..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/VridhiJ/CIS519/refs/heads/main/Dataset/news_urls.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Display the first few rows to verify the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3805 entries, 0 to 3804\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   url     3805 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 29.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Headline Collection Method\n",
    "\n",
    "Collect the news headlines by scraping multiple news websites using BeautifulSoup libraries. The scraping process involved:\n",
    "\n",
    "1. Fetching Webpages:\n",
    "\n",
    "  - Sending HTTP requests to news article URLs.\n",
    "\n",
    "  - Using appropriate headers to mimic a real browser and avoid blocking.\n",
    "    - User-Agent: Identifies the client making request. Helps avoid bot detection by mimicking real browser behavior.\n",
    "    - Accept-Charset:  Specifies the character encodings that the client can process. Helps ensure proper text rendering.\n",
    "    - Accept: Defines the type of content the client expects from the server.\n",
    "    - Accept-Language: Specifies the preferred language for the response content. Helps receive content in a readable format when a website supports multiple languages.\n",
    "    - referer: Indicates the URL of the page that made the request.\n",
    "    \n",
    "\n",
    "2. Extracting Headlines:\n",
    "\n",
    "  - Parsing the webpage content with BeautifulSoup.\n",
    "\n",
    "  - Identifying and extracting headlines using H1 tags and class attributes related to headlines.\n",
    "\n",
    "  - Handling variations in website structures dynamically.\n",
    "\n",
    "3. Error Handling & Optimization:\n",
    "\n",
    "  - Implementing error handling to skip unavailable pages.\n",
    "\n",
    "4. Storing Data:\n",
    "\n",
    "  - Storing extracted headlines in a structured pandas DataFrame.\n",
    "\n",
    " - Saving the data in CSV format for further processing.\n",
    "\n",
    "This method ensures efficient and scalable data collection while minimizing disruptions caused by website restrictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Scraping (don't rerun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get headline from a single URL\n",
    "def get_article_headline(url):\n",
    "  try:\n",
    "    user_agents = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',\n",
    "    ]\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    headers = {\n",
    "    'user-agent': random.choice(user_agents),\n",
    "    \"Accept-Charset\": \"utf-8\",\n",
    "    \"Accept\": \"*/*\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"referer\": \"https://www.google.com/\",\n",
    "    }\n",
    "    time.sleep(2)\n",
    "\n",
    "    response = requests.get(url, headers = headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "      print(f\"Warning: Failed to load page {url} (Status Code: {response.status_code})\")\n",
    "      return None  # Don't stop execution, just return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # To find headline of various types of classes\n",
    "    headline = soup.find(\"h1\", class_=lambda c: c and \"headline\" in c)\n",
    "\n",
    "    if headline:\n",
    "      headline = ftfy.fix_text(headline.get_text())  # Fix any encoding issues\n",
    "      return headline.strip()  # Return the cleaned headline\n",
    "    else:\n",
    "      return None  # Return None if no headline is found\n",
    "  except Exception as e:\n",
    "    print(f\"Error processing {url}: {e}\")\n",
    "    return None  # Return None in case of an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to load page https://www.nbcnews.com/feature/nbc-out/lil-nas-x-dolly-parton-lena-waithe-appear-virtual-glaad-n1233424 (Status Code: 404)\n",
      "Warning: Failed to load page https://www.nbcnews.com/select/shopping/what-we-bought-black-friday-cyber-monday-rcna126998 (Status Code: 500)\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list to store the headlines\n",
    "headlines = []\n",
    "\n",
    "# Loop through the URLs in your dataframe\n",
    "for url in df['url']:\n",
    "    headline = get_article_headline(url)\n",
    "    headlines.append(headline)\n",
    "\n",
    "# Add the scraped headlines to your dataframe\n",
    "df['headline'] = headlines\n",
    "\n",
    "# Show the first few rows with the scraped headlines\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"scraped_headlines.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import create_repo\n",
    "\n",
    "# Create a repository on Hugging Face Hub\n",
    "repo_name = 'scraped-headlines'\n",
    "create_repo(repo_name, private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import upload_file\n",
    "\n",
    "upload_file(\n",
    "    path_or_fileobj='scraped_headlines.csv',\n",
    "    path_in_repo='scraped_headlines_v4.csv',\n",
    "    repo_id= 'VridhiJain/scraped-headlines'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
