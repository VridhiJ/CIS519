{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ftfy huggingface_hub scikit-learn transformers datasets optuna --quiet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import ftfy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.foxnews.com/lifestyle/jack-carrs-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.foxnews.com/entertainment/bruce-wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.foxnews.com/politics/blinken-meets...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.foxnews.com/entertainment/emily-bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.foxnews.com/media/the-view-co-host...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url\n",
       "0  https://www.foxnews.com/lifestyle/jack-carrs-e...\n",
       "1  https://www.foxnews.com/entertainment/bruce-wi...\n",
       "2  https://www.foxnews.com/politics/blinken-meets...\n",
       "3  https://www.foxnews.com/entertainment/emily-bl...\n",
       "4  https://www.foxnews.com/media/the-view-co-host..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/VridhiJ/CIS519/refs/heads/main/Dataset/news_urls.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Display the first few rows to verify the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3805 entries, 0 to 3804\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   url     3805 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 29.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Headline Collection Method\n",
    "\n",
    "Collect the news headlines by scraping multiple news websites using BeautifulSoup libraries. The scraping process involved:\n",
    "\n",
    "1. Fetching Webpages:\n",
    "\n",
    "  - Sending HTTP requests to news article URLs.\n",
    "\n",
    "  - Using appropriate headers to mimic a real browser and avoid blocking.\n",
    "    - User-Agent: Identifies the client making request. Helps avoid bot detection by mimicking real browser behavior.\n",
    "    - Accept-Charset:  Specifies the character encodings that the client can process. Helps ensure proper text rendering.\n",
    "    - Accept: Defines the type of content the client expects from the server.\n",
    "    - Accept-Language: Specifies the preferred language for the response content. Helps receive content in a readable format when a website supports multiple languages.\n",
    "    - referer: Indicates the URL of the page that made the request.\n",
    "    \n",
    "\n",
    "2. Extracting Headlines:\n",
    "\n",
    "  - Parsing the webpage content with BeautifulSoup.\n",
    "\n",
    "  - Identifying and extracting headlines using H1 tags and class attributes related to headlines.\n",
    "\n",
    "  - Handling variations in website structures dynamically.\n",
    "\n",
    "3. Error Handling & Optimization:\n",
    "\n",
    "  - Implementing error handling to skip unavailable pages.\n",
    "\n",
    "4. Storing Data:\n",
    "\n",
    "  - Storing extracted headlines in a structured pandas DataFrame.\n",
    "\n",
    " - Saving the data in CSV format for further processing.\n",
    "\n",
    "This method ensures efficient and scalable data collection while minimizing disruptions caused by website restrictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Scraping (don't rerun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get headline from a single URL\n",
    "def get_article_headline(url):\n",
    "  try:\n",
    "    user_agents = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',\n",
    "    ]\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    headers = {\n",
    "    'user-agent': random.choice(user_agents),\n",
    "    \"Accept-Charset\": \"utf-8\",\n",
    "    \"Accept\": \"*/*\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"referer\": \"https://www.google.com/\",\n",
    "    }\n",
    "    time.sleep(2)\n",
    "\n",
    "    response = requests.get(url, headers = headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "      print(f\"Warning: Failed to load page {url} (Status Code: {response.status_code})\")\n",
    "      return None  # Don't stop execution, just return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # To find headline of various types of classes\n",
    "    headline = soup.find(\"h1\", class_=lambda c: c and \"headline\" in c)\n",
    "\n",
    "    if headline:\n",
    "      headline = ftfy.fix_text(headline.get_text())  # Fix any encoding issues\n",
    "      return headline.strip()  # Return the cleaned headline\n",
    "    else:\n",
    "      return None  # Return None if no headline is found\n",
    "  except Exception as e:\n",
    "    print(f\"Error processing {url}: {e}\")\n",
    "    return None  # Return None in case of an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to load page https://www.nbcnews.com/feature/nbc-out/lil-nas-x-dolly-parton-lena-waithe-appear-virtual-glaad-n1233424 (Status Code: 404)\n",
      "Warning: Failed to load page https://www.nbcnews.com/select/shopping/what-we-bought-black-friday-cyber-monday-rcna126998 (Status Code: 500)\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list to store the headlines\n",
    "headlines = []\n",
    "\n",
    "# Loop through the URLs in your dataframe\n",
    "for url in df['url']:\n",
    "    headline = get_article_headline(url)\n",
    "    headlines.append(headline)\n",
    "\n",
    "# Add the scraped headlines to your dataframe\n",
    "df['headline'] = headlines\n",
    "\n",
    "# Show the first few rows with the scraped headlines\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"scraped_headlines.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import create_repo\n",
    "\n",
    "# Create a repository on Hugging Face Hub\n",
    "repo_name = 'scraped-headlines'\n",
    "create_repo(repo_name, private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import upload_file\n",
    "\n",
    "upload_file(\n",
    "    path_or_fileobj='scraped_headlines.csv',\n",
    "    path_in_repo='scraped_headlines_v4.csv',\n",
    "    repo_id= 'VridhiJain/scraped-headlines'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()  # enter your Hugging Face token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.foxnews.com/lifestyle/jack-carrs-e...</td>\n",
       "      <td>Jack Carr recalls Gen. Eisenhower's D-Day memo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.foxnews.com/entertainment/bruce-wi...</td>\n",
       "      <td>Bruce Willis, Demi Moore avoided doing one thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.foxnews.com/politics/blinken-meets...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.foxnews.com/entertainment/emily-bl...</td>\n",
       "      <td>Emily Blunt says her 'toes curl' when people t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.foxnews.com/media/the-view-co-host...</td>\n",
       "      <td>'The View' co-host, CNN commentator Ana Navarr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://www.foxnews.com/lifestyle/jack-carrs-e...   \n",
       "1  https://www.foxnews.com/entertainment/bruce-wi...   \n",
       "2  https://www.foxnews.com/politics/blinken-meets...   \n",
       "3  https://www.foxnews.com/entertainment/emily-bl...   \n",
       "4  https://www.foxnews.com/media/the-view-co-host...   \n",
       "\n",
       "                                            headline  \n",
       "0  Jack Carr recalls Gen. Eisenhower's D-Day memo...  \n",
       "1  Bruce Willis, Demi Moore avoided doing one thi...  \n",
       "2                                                NaN  \n",
       "3  Emily Blunt says her 'toes curl' when people t...  \n",
       "4  'The View' co-host, CNN commentator Ana Navarr...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "repo_id = \"VridhiJain/scraped-headlines\"  # repo name\n",
    "filename = \"scraped_headlines_v4.csv\"  # file name\n",
    "\n",
    "# Download the file\n",
    "file_path = hf_hub_download(repo_id=repo_id, filename=filename)\n",
    "\n",
    "# Load into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3805 entries, 0 to 3804\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   url       3805 non-null   object\n",
      " 1   headline  3352 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 59.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url           0\n",
      "headline    453\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in the dataset\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Drop any rows where the headline is missing/duplicates\n",
    "df = df.dropna(subset=['headline']).drop_duplicates(subset=['headline'])\n",
    "\n",
    "# Reset index after dropping rows\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3336 entries, 0 to 3335\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   url       3336 non-null   object\n",
      " 1   headline  3336 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 52.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "url\n",
       "False    1779\n",
       "True     1557\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['url'].str.contains('foxnews').value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fox News Headlines: 1779\n",
    "\n",
    "NBC News Headlines: 1557"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model(TF-IDF + Log Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if using GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd9bf3f74c24241a8f8aa4ef25b3ef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scraped_headlines_v2.csv:   0%|          | 0.00/687k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6682\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.77      0.70       333\n",
      "           1       0.71      0.57      0.63       330\n",
      "\n",
      "    accuracy                           0.67       663\n",
      "   macro avg       0.67      0.67      0.66       663\n",
      "weighted avg       0.67      0.67      0.66       663\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the preprocessed headline data from Hugging Face\n",
    "from huggingface_hub import hf_hub_download\n",
    "csv_path = hf_hub_download(repo_id=\"VridhiJain/scraped-headlines\", filename=\"scraped_headlines_v2.csv\")\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Drop rows with missing headlines\n",
    "df = df.dropna(subset=['headline'])\n",
    "\n",
    "# Label: 1 for FoxNews, 0 for NBC\n",
    "df['label'] = df['url'].apply(lambda x: 1 if \"foxnews\" in x else 0)\n",
    "\n",
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['headline'], df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=100)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Train Logistic Regression\n",
    "baseline_model = LogisticRegression(max_iter=100)\n",
    "baseline_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = baseline_model.predict(X_test_tfidf)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
