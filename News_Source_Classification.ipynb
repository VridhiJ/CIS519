{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ftfy huggingface_hub scikit-learn transformers datasets optuna accelerate==0.27.2 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import ftfy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.foxnews.com/lifestyle/jack-carrs-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.foxnews.com/entertainment/bruce-wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.foxnews.com/politics/blinken-meets...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.foxnews.com/entertainment/emily-bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.foxnews.com/media/the-view-co-host...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url\n",
       "0  https://www.foxnews.com/lifestyle/jack-carrs-e...\n",
       "1  https://www.foxnews.com/entertainment/bruce-wi...\n",
       "2  https://www.foxnews.com/politics/blinken-meets...\n",
       "3  https://www.foxnews.com/entertainment/emily-bl...\n",
       "4  https://www.foxnews.com/media/the-view-co-host..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/VridhiJ/CIS519/refs/heads/main/Dataset/news_urls.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Display the first few rows to verify the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3805 entries, 0 to 3804\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   url     3805 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 29.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Headline Collection Method\n",
    "\n",
    "Collect the news headlines by scraping multiple news websites using BeautifulSoup libraries. The scraping process involved:\n",
    "\n",
    "1. Fetching Webpages:\n",
    "\n",
    "  - Sending HTTP requests to news article URLs.\n",
    "\n",
    "  - Using appropriate headers to mimic a real browser and avoid blocking.\n",
    "    - User-Agent: Identifies the client making request. Helps avoid bot detection by mimicking real browser behavior.\n",
    "    - Accept-Charset:  Specifies the character encodings that the client can process. Helps ensure proper text rendering.\n",
    "    - Accept: Defines the type of content the client expects from the server.\n",
    "    - Accept-Language: Specifies the preferred language for the response content. Helps receive content in a readable format when a website supports multiple languages.\n",
    "    - referer: Indicates the URL of the page that made the request.\n",
    "    \n",
    "\n",
    "2. Extracting Headlines:\n",
    "\n",
    "  - Parsing the webpage content with BeautifulSoup.\n",
    "\n",
    "  - Identifying and extracting headlines using H1 tags and class attributes related to headlines.\n",
    "\n",
    "  - Handling variations in website structures dynamically.\n",
    "\n",
    "3. Error Handling & Optimization:\n",
    "\n",
    "  - Implementing error handling to skip unavailable pages.\n",
    "\n",
    "4. Storing Data:\n",
    "\n",
    "  - Storing extracted headlines in a structured pandas DataFrame.\n",
    "\n",
    " - Saving the data in CSV format for further processing.\n",
    "\n",
    "This method ensures efficient and scalable data collection while minimizing disruptions caused by website restrictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Scraping (don't rerun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get headline from a single URL\n",
    "def get_article_headline(url):\n",
    "  try:\n",
    "    user_agents = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',\n",
    "    ]\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    headers = {\n",
    "    'user-agent': random.choice(user_agents),\n",
    "    \"Accept-Charset\": \"utf-8\",\n",
    "    \"Accept\": \"*/*\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"referer\": \"https://www.google.com/\",\n",
    "    }\n",
    "    time.sleep(2)\n",
    "\n",
    "    response = requests.get(url, headers = headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "      print(f\"Warning: Failed to load page {url} (Status Code: {response.status_code})\")\n",
    "      return None  # Don't stop execution, just return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # To find headline of various types of classes\n",
    "    headline = soup.find(\"h1\", class_=lambda c: c and \"headline\" in c)\n",
    "\n",
    "    if headline:\n",
    "      headline = ftfy.fix_text(headline.get_text())  # Fix any encoding issues\n",
    "      return headline.strip()  # Return the cleaned headline\n",
    "    else:\n",
    "      return None  # Return None if no headline is found\n",
    "  except Exception as e:\n",
    "    print(f\"Error processing {url}: {e}\")\n",
    "    return None  # Return None in case of an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to load page https://www.nbcnews.com/feature/nbc-out/lil-nas-x-dolly-parton-lena-waithe-appear-virtual-glaad-n1233424 (Status Code: 404)\n",
      "Warning: Failed to load page https://www.nbcnews.com/select/shopping/what-we-bought-black-friday-cyber-monday-rcna126998 (Status Code: 500)\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list to store the headlines\n",
    "headlines = []\n",
    "\n",
    "# Loop through the URLs in your dataframe\n",
    "for url in df['url']:\n",
    "    headline = get_article_headline(url)\n",
    "    headlines.append(headline)\n",
    "\n",
    "# Add the scraped headlines to your dataframe\n",
    "df['headline'] = headlines\n",
    "\n",
    "# Show the first few rows with the scraped headlines\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"scraped_headlines.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import create_repo\n",
    "\n",
    "# Create a repository on Hugging Face Hub\n",
    "repo_name = 'scraped-headlines'\n",
    "create_repo(repo_name, private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import upload_file\n",
    "\n",
    "upload_file(\n",
    "    path_or_fileobj='scraped_headlines.csv',\n",
    "    path_in_repo='scraped_headlines_v4.csv',\n",
    "    repo_id= 'VridhiJain/scraped-headlines'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66a2916b8a774a1599ff62a1a1e09c0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()  # enter your Hugging Face token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aba9ca2b5da34efeaed919b6ed4dee5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scraped_headlines_v4.csv:   0%|          | 0.00/689k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.foxnews.com/lifestyle/jack-carrs-e...</td>\n",
       "      <td>Jack Carr recalls Gen. Eisenhower's D-Day memo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.foxnews.com/entertainment/bruce-wi...</td>\n",
       "      <td>Bruce Willis, Demi Moore avoided doing one thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.foxnews.com/politics/blinken-meets...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.foxnews.com/entertainment/emily-bl...</td>\n",
       "      <td>Emily Blunt says her 'toes curl' when people t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.foxnews.com/media/the-view-co-host...</td>\n",
       "      <td>'The View' co-host, CNN commentator Ana Navarr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://www.foxnews.com/lifestyle/jack-carrs-e...   \n",
       "1  https://www.foxnews.com/entertainment/bruce-wi...   \n",
       "2  https://www.foxnews.com/politics/blinken-meets...   \n",
       "3  https://www.foxnews.com/entertainment/emily-bl...   \n",
       "4  https://www.foxnews.com/media/the-view-co-host...   \n",
       "\n",
       "                                            headline  \n",
       "0  Jack Carr recalls Gen. Eisenhower's D-Day memo...  \n",
       "1  Bruce Willis, Demi Moore avoided doing one thi...  \n",
       "2                                                NaN  \n",
       "3  Emily Blunt says her 'toes curl' when people t...  \n",
       "4  'The View' co-host, CNN commentator Ana Navarr...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "repo_id = \"VridhiJain/scraped-headlines\"  # repo name\n",
    "filename = \"scraped_headlines_v4.csv\"  # file name\n",
    "\n",
    "# Download the file\n",
    "file_path = hf_hub_download(repo_id=repo_id, filename=filename)\n",
    "\n",
    "# Load into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3805 entries, 0 to 3804\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   url       3805 non-null   object\n",
      " 1   headline  3352 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 59.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url           0\n",
      "headline    453\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in the dataset\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Drop any rows where the headline is missing/duplicates\n",
    "df = df.dropna(subset=['headline']).drop_duplicates(subset=['headline'])\n",
    "\n",
    "# Reset index after dropping rows\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3336 entries, 0 to 3335\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   url       3336 non-null   object\n",
      " 1   headline  3336 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 52.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "url\n",
       "False    1779\n",
       "True     1557\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['url'].str.contains('foxnews').value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fox News Headlines: 1779\n",
    "\n",
    "NBC News Headlines: 1557"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model(TF-IDF + Log Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if using GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bc43d35279648018c3dd706335bc18d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scraped_headlines_v4.csv:   0%|          | 0.00/689k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7036\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.65      0.69       335\n",
      "           1       0.68      0.76      0.72       333\n",
      "\n",
      "    accuracy                           0.70       668\n",
      "   macro avg       0.71      0.70      0.70       668\n",
      "weighted avg       0.71      0.70      0.70       668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the preprocessed headline data from Hugging Face\n",
    "from huggingface_hub import hf_hub_download\n",
    "csv_path = hf_hub_download(repo_id=\"VridhiJain/scraped-headlines\", filename=\"scraped_headlines_v4.csv\")\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Drop rows with missing headlines\n",
    "df = df.dropna(subset=['headline']).drop_duplicates(subset=['headline'])\n",
    "\n",
    "# Label: 1 for FoxNews, 0 for NBC\n",
    "df['label'] = df['url'].apply(lambda x: 1 if \"foxnews\" in x else 0)\n",
    "\n",
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['headline'], df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=100)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Train Logistic Regression\n",
    "baseline_model = LogisticRegression(max_iter=100)\n",
    "baseline_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = baseline_model.predict(X_test_tfidf)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert-based Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from datasets import Dataset\n",
    "from transformers import (AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6eea928a7924d8c929469bf2331f1d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c54a4b274de4d04b83bd9c87a12291f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43c887ff518a4bbd8e1a5ed6e68d6716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b29f140a04e94d0d90ef8c7f5e9f69ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b3b33bea89b4734b68f80c8c1011c5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4486e7eb669479ca44bd9bac6e3e086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ec7ff1925e84bd789174edc2dec1192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='501' max='501' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [501/501 03:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.426632</td>\n",
       "      <td>0.815868</td>\n",
       "      <td>0.793970</td>\n",
       "      <td>0.831579</td>\n",
       "      <td>0.759615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.419494</td>\n",
       "      <td>0.823353</td>\n",
       "      <td>0.819018</td>\n",
       "      <td>0.785294</td>\n",
       "      <td>0.855769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.359400</td>\n",
       "      <td>0.455817</td>\n",
       "      <td>0.841317</td>\n",
       "      <td>0.816609</td>\n",
       "      <td>0.887218</td>\n",
       "      <td>0.756410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='42' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [42/42 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Evaluation Results:\n",
      "{'eval_loss': 0.4558168649673462, 'eval_accuracy': 0.8413173652694611, 'eval_f1': 0.8166089965397924, 'eval_precision': 0.8872180451127819, 'eval_recall': 0.7564102564102564, 'eval_runtime': 5.7073, 'eval_samples_per_second': 117.043, 'eval_steps_per_second': 7.359, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "df['label'] = df['url'].apply(lambda x: 1 if \"foxnews\" in x.lower() else 0)\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df[['headline', 'label']])\n",
    "test_dataset = Dataset.from_pandas(test_df[['headline', 'label']])\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"headline\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# Load model\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Define evaluation metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = torch.argmax(torch.tensor(pred.predictions), axis=1).numpy()\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return{\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=bert_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "results = trainer.evaluate()\n",
    "print(\"BERT Evaluation Results:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/VridhiJain/bert_vanilla_2', endpoint='https://huggingface.co', repo_type='model', repo_id='VridhiJain/bert_vanilla_2')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "# Repo name\n",
    "repo_name = \"bert_vanilla_2\"\n",
    "\n",
    "# This creates a repo under your namespace (username)\n",
    "api.create_repo(repo_id=repo_name, private=False, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login, login\n",
    "\n",
    "notebook_login()  # enter your Hugging Face token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push model and tokenizer\n",
    "trainer.model.push_to_hub(\"bert_vanilla_2\")\n",
    "tokenizer.push_to_hub(\"bert_vanilla_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa-based Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36ea8d9e7db5463d9db1a5200678e5ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "241bd1a66216414595f472c0a5000c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0979aace5b564b1795ac2ad6cd5cede0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20533d9034e344b0bc4b97bc129992ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa3279b3267546d09e511478ab347c19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f98a9d971e394d83b0cc31e11d208ad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20062e076c754ac790ad4ceb10966077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd3af371c11842bf9c3c9a031160bec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='501' max='501' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [501/501 03:49, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.564700</td>\n",
       "      <td>0.428925</td>\n",
       "      <td>0.806886</td>\n",
       "      <td>0.774869</td>\n",
       "      <td>0.850575</td>\n",
       "      <td>0.711538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.351700</td>\n",
       "      <td>0.384277</td>\n",
       "      <td>0.850299</td>\n",
       "      <td>0.844720</td>\n",
       "      <td>0.819277</td>\n",
       "      <td>0.871795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.222100</td>\n",
       "      <td>0.380195</td>\n",
       "      <td>0.860778</td>\n",
       "      <td>0.848780</td>\n",
       "      <td>0.861386</td>\n",
       "      <td>0.836538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='42' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [42/42 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa Evaluation Results: {'eval_loss': 0.3801954686641693, 'eval_accuracy': 0.8607784431137725, 'eval_f1': 0.848780487804878, 'eval_precision': 0.8613861386138614, 'eval_recall': 0.8365384615384616, 'eval_runtime': 5.7908, 'eval_samples_per_second': 115.356, 'eval_steps_per_second': 7.253, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "tokenizer_roberta = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "train_dataset = Dataset.from_pandas(train_df[[\"headline\", \"label\"]])\n",
    "test_dataset = Dataset.from_pandas(test_df[[\"headline\", \"label\"]])\n",
    "\n",
    "def tokenize_roberta(batch):\n",
    "    return tokenizer_roberta(batch[\"headline\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_roberta, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_roberta, batched=True)\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "roberta_model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "\n",
    "training_args_roberta = TrainingArguments(\n",
    "    output_dir=\"./roberta_results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer_roberta = Trainer(\n",
    "    model=roberta_model,\n",
    "    args=training_args_roberta,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer_roberta.train()\n",
    "roberta_results = trainer_roberta.evaluate()\n",
    "print(\"RoBERTa Evaluation Results:\", roberta_results)\n",
    "\n",
    "trainer_roberta.save_model(\"./roberta_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/VridhiJain/roberta_vanilla_2', endpoint='https://huggingface.co', repo_type='model', repo_id='VridhiJain/roberta_vanilla_2')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Repo name\n",
    "repo_name = \"roberta_vanilla_2\"\n",
    "\n",
    "# This creates a repo under your namespace (username)\n",
    "api.create_repo(repo_id=repo_name, private=False, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push model and tokenizer\n",
    "trainer.model.push_to_hub(\"roberta_vanilla_2\")\n",
    "tokenizer.push_to_hub(\"roberta_vanilla_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert-based Classifier - Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fba15134fe54ff9bec4043b727b4798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfeeb34f8ef14dedb54098b6ea35185b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "998c39157a694e989646acfaf4595ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c664dfca2e44f43911114d4751ee8a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e1b9ab2fd4341888104b79e07f62338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "495d223b3c0241c5983b2b6713d432a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df['label'] = df['url'].apply(lambda x: 1 if \"foxnews\" in x.lower() else 0)\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df[['headline', 'label']])\n",
    "test_dataset = Dataset.from_pandas(test_df[['headline', 'label']])\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"headline\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "  labels = pred.label_ids\n",
    "  preds = torch.argmax(torch.tensor(pred.predictions), axis=1).numpy()\n",
    "  precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "  acc = accuracy_score(labels, preds)\n",
    "  return{\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with lr=2e-05, epochs=3, weight_decay=0.01\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='501' max='501' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [501/501 07:47, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.426632</td>\n",
       "      <td>0.815868</td>\n",
       "      <td>0.793970</td>\n",
       "      <td>0.831579</td>\n",
       "      <td>0.759615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.419494</td>\n",
       "      <td>0.823353</td>\n",
       "      <td>0.819018</td>\n",
       "      <td>0.785294</td>\n",
       "      <td>0.855769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.359400</td>\n",
       "      <td>0.455817</td>\n",
       "      <td>0.841317</td>\n",
       "      <td>0.816609</td>\n",
       "      <td>0.887218</td>\n",
       "      <td>0.756410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='42' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [42/42 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.8166089965397924\n",
      "\n",
      "Training with lr=2e-05, epochs=3, weight_decay=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='501' max='501' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [501/501 07:47, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.442123</td>\n",
       "      <td>0.808383</td>\n",
       "      <td>0.779310</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.724359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.434572</td>\n",
       "      <td>0.808383</td>\n",
       "      <td>0.802469</td>\n",
       "      <td>0.773810</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.380300</td>\n",
       "      <td>0.449917</td>\n",
       "      <td>0.833832</td>\n",
       "      <td>0.807626</td>\n",
       "      <td>0.879245</td>\n",
       "      <td>0.746795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='42' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [42/42 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.8076256499133448\n",
      "\n",
      "Training with lr=2e-05, epochs=4, weight_decay=0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='668' max='668' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [668/668 10:23, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.469424</td>\n",
       "      <td>0.773952</td>\n",
       "      <td>0.718808</td>\n",
       "      <td>0.857778</td>\n",
       "      <td>0.618590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.429148</td>\n",
       "      <td>0.829341</td>\n",
       "      <td>0.815534</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.807692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.382300</td>\n",
       "      <td>0.463697</td>\n",
       "      <td>0.832335</td>\n",
       "      <td>0.808219</td>\n",
       "      <td>0.867647</td>\n",
       "      <td>0.756410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.382300</td>\n",
       "      <td>0.536257</td>\n",
       "      <td>0.833832</td>\n",
       "      <td>0.810903</td>\n",
       "      <td>0.865455</td>\n",
       "      <td>0.762821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='42' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [42/42 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.8109028960817717\n",
      "\n",
      "Training with lr=2e-05, epochs=4, weight_decay=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='668' max='668' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [668/668 10:23, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.427433</td>\n",
       "      <td>0.808383</td>\n",
       "      <td>0.775439</td>\n",
       "      <td>0.856589</td>\n",
       "      <td>0.708333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.414941</td>\n",
       "      <td>0.829341</td>\n",
       "      <td>0.826220</td>\n",
       "      <td>0.787791</td>\n",
       "      <td>0.868590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.343500</td>\n",
       "      <td>0.452554</td>\n",
       "      <td>0.839820</td>\n",
       "      <td>0.823140</td>\n",
       "      <td>0.849829</td>\n",
       "      <td>0.798077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.343500</td>\n",
       "      <td>0.571782</td>\n",
       "      <td>0.839820</td>\n",
       "      <td>0.819562</td>\n",
       "      <td>0.864769</td>\n",
       "      <td>0.778846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='42' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [42/42 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.8195615514333895\n",
      "\n",
      "Training with lr=2e-05, epochs=5, weight_decay=0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='835' max='835' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [835/835 12:58, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.429466</td>\n",
       "      <td>0.811377</td>\n",
       "      <td>0.770909</td>\n",
       "      <td>0.890756</td>\n",
       "      <td>0.679487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.413370</td>\n",
       "      <td>0.832335</td>\n",
       "      <td>0.819936</td>\n",
       "      <td>0.822581</td>\n",
       "      <td>0.817308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.336200</td>\n",
       "      <td>0.489701</td>\n",
       "      <td>0.838323</td>\n",
       "      <td>0.824675</td>\n",
       "      <td>0.835526</td>\n",
       "      <td>0.814103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.336200</td>\n",
       "      <td>0.660159</td>\n",
       "      <td>0.823353</td>\n",
       "      <td>0.787770</td>\n",
       "      <td>0.897541</td>\n",
       "      <td>0.701923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.336200</td>\n",
       "      <td>0.686295</td>\n",
       "      <td>0.842814</td>\n",
       "      <td>0.824708</td>\n",
       "      <td>0.860627</td>\n",
       "      <td>0.791667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='42' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [42/42 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.8247078464106845\n",
      "\n",
      "Training with lr=2e-05, epochs=5, weight_decay=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='835' max='835' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [835/835 12:59, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.453365</td>\n",
       "      <td>0.794910</td>\n",
       "      <td>0.751361</td>\n",
       "      <td>0.866109</td>\n",
       "      <td>0.663462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.401113</td>\n",
       "      <td>0.826347</td>\n",
       "      <td>0.806020</td>\n",
       "      <td>0.842657</td>\n",
       "      <td>0.772436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.347700</td>\n",
       "      <td>0.464564</td>\n",
       "      <td>0.832335</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.862319</td>\n",
       "      <td>0.762821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.347700</td>\n",
       "      <td>0.677387</td>\n",
       "      <td>0.845808</td>\n",
       "      <td>0.815742</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.730769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.347700</td>\n",
       "      <td>0.713668</td>\n",
       "      <td>0.835329</td>\n",
       "      <td>0.809028</td>\n",
       "      <td>0.882576</td>\n",
       "      <td>0.746795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='42' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [42/42 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.8090277777777778\n",
      "\n",
      "Training with lr=3e-05, epochs=3, weight_decay=0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='458' max='501' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [458/501 06:56 < 00:39, 1.10 it/s, Epoch 2.74/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.446268</td>\n",
       "      <td>0.800898</td>\n",
       "      <td>0.773424</td>\n",
       "      <td>0.825455</td>\n",
       "      <td>0.727564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.399312</td>\n",
       "      <td>0.835329</td>\n",
       "      <td>0.818482</td>\n",
       "      <td>0.843537</td>\n",
       "      <td>0.794872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='42' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [42/42 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.7902097902097902\n",
      "\n",
      "Training with lr=3e-05, epochs=4, weight_decay=0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='668' max='668' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [668/668 10:23, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.446978</td>\n",
       "      <td>0.800898</td>\n",
       "      <td>0.765432</td>\n",
       "      <td>0.850980</td>\n",
       "      <td>0.695513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.466851</td>\n",
       "      <td>0.812874</td>\n",
       "      <td>0.776386</td>\n",
       "      <td>0.878543</td>\n",
       "      <td>0.695513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.327600</td>\n",
       "      <td>0.579543</td>\n",
       "      <td>0.827844</td>\n",
       "      <td>0.800693</td>\n",
       "      <td>0.871698</td>\n",
       "      <td>0.740385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.327600</td>\n",
       "      <td>0.702011</td>\n",
       "      <td>0.835329</td>\n",
       "      <td>0.810997</td>\n",
       "      <td>0.874074</td>\n",
       "      <td>0.756410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='42' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [42/42 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.8109965635738832\n",
      "\n",
      "Training with lr=3e-05, epochs=4, weight_decay=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='668' max='668' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [668/668 10:22, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.434977</td>\n",
       "      <td>0.802395</td>\n",
       "      <td>0.764286</td>\n",
       "      <td>0.862903</td>\n",
       "      <td>0.685897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.424984</td>\n",
       "      <td>0.830838</td>\n",
       "      <td>0.810084</td>\n",
       "      <td>0.851590</td>\n",
       "      <td>0.772436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.310400</td>\n",
       "      <td>0.551798</td>\n",
       "      <td>0.853293</td>\n",
       "      <td>0.833898</td>\n",
       "      <td>0.884892</td>\n",
       "      <td>0.788462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.310400</td>\n",
       "      <td>0.657122</td>\n",
       "      <td>0.857784</td>\n",
       "      <td>0.841402</td>\n",
       "      <td>0.878049</td>\n",
       "      <td>0.807692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='42' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [42/42 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.8414023372287145\n",
      "\n",
      "Training with lr=3e-05, epochs=5, weight_decay=0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='835' max='835' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [835/835 12:58, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.426798</td>\n",
       "      <td>0.797904</td>\n",
       "      <td>0.764398</td>\n",
       "      <td>0.839080</td>\n",
       "      <td>0.701923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.416095</td>\n",
       "      <td>0.848802</td>\n",
       "      <td>0.836834</td>\n",
       "      <td>0.843648</td>\n",
       "      <td>0.830128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.315100</td>\n",
       "      <td>0.563682</td>\n",
       "      <td>0.845808</td>\n",
       "      <td>0.821490</td>\n",
       "      <td>0.894340</td>\n",
       "      <td>0.759615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.315100</td>\n",
       "      <td>0.773806</td>\n",
       "      <td>0.850299</td>\n",
       "      <td>0.822695</td>\n",
       "      <td>0.920635</td>\n",
       "      <td>0.743590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.315100</td>\n",
       "      <td>0.745173</td>\n",
       "      <td>0.856287</td>\n",
       "      <td>0.837288</td>\n",
       "      <td>0.888489</td>\n",
       "      <td>0.791667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='42' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [42/42 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.8372881355932204\n",
      "\n",
      "Training with lr=3e-05, epochs=5, weight_decay=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='835' max='835' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [835/835 12:59, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.471030</td>\n",
       "      <td>0.782934</td>\n",
       "      <td>0.741533</td>\n",
       "      <td>0.835341</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.400084</td>\n",
       "      <td>0.848802</td>\n",
       "      <td>0.834154</td>\n",
       "      <td>0.855219</td>\n",
       "      <td>0.814103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.318400</td>\n",
       "      <td>0.694243</td>\n",
       "      <td>0.821856</td>\n",
       "      <td>0.780847</td>\n",
       "      <td>0.917749</td>\n",
       "      <td>0.679487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.318400</td>\n",
       "      <td>0.896279</td>\n",
       "      <td>0.821856</td>\n",
       "      <td>0.780037</td>\n",
       "      <td>0.921397</td>\n",
       "      <td>0.676282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.318400</td>\n",
       "      <td>0.863166</td>\n",
       "      <td>0.827844</td>\n",
       "      <td>0.801382</td>\n",
       "      <td>0.868914</td>\n",
       "      <td>0.743590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='42' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [42/42 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.8013816925734024\n",
      "\n",
      "Training with lr=5e-05, epochs=3, weight_decay=0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='501' max='501' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [501/501 07:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.398506</td>\n",
       "      <td>0.802395</td>\n",
       "      <td>0.766784</td>\n",
       "      <td>0.854331</td>\n",
       "      <td>0.695513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.383496</td>\n",
       "      <td>0.863772</td>\n",
       "      <td>0.853462</td>\n",
       "      <td>0.857605</td>\n",
       "      <td>0.849359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.286300</td>\n",
       "      <td>0.526548</td>\n",
       "      <td>0.854790</td>\n",
       "      <td>0.834188</td>\n",
       "      <td>0.893773</td>\n",
       "      <td>0.782051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [28/42 00:07 < 00:03, 3.55 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'https://jupyter-labs-8888-1746565671239881544.cluster1.service-inference.ai/'. Verify the server is running and reachable."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import (AutoModelForSequenceClassification, Trainer, TrainingArguments)\n",
    "import numpy as np\n",
    "\n",
    "# define search space\n",
    "learning_rates = [2e-5, 3e-5, 5e-5]\n",
    "epochs = [3, 4, 5]\n",
    "weight_decays = [0.01, 0.001]\n",
    "\n",
    "best_f1 = 0\n",
    "best_config = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "  for num_epochs in epochs:\n",
    "    for wd in weight_decays:\n",
    "      print(f\"\\nTraining with lr={lr}, epochs={num_epochs}, weight_decay={wd}\")\n",
    "\n",
    "      training_args = TrainingArguments(\n",
    "        output_dir=f\"./bert_tuned_lr{lr}_ep{num_epochs}_wd{wd}\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=num_epochs,\n",
    "        weight_decay=wd,\n",
    "        report_to=\"none\"\n",
    "      )\n",
    "\n",
    "      model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "      trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        compute_metrics=compute_metrics\n",
    "      )\n",
    "\n",
    "      trainer.train()\n",
    "      eval_results = trainer.evaluate()\n",
    "      print(\"F1 score:\", eval_results['eval_f1'])\n",
    "\n",
    "      if eval_results['eval_f1'] > best_f1:\n",
    "        best_f1 = eval_results['eval_f1']\n",
    "        best_config = {\n",
    "            \"learning_rate\": lr,\n",
    "            \"num_epochs\": num_epochs,\n",
    "            \"weight_decay\": wd,\n",
    "            \"eval_results\": eval_results\n",
    "        }\n",
    "\n",
    "print(\"\\nBest configuration:\")\n",
    "print(best_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert-based Classifier retrained w/ best hyperparameters (from grid search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best configuration (best F1):\n",
    "- Learning rate: 5e-05 \n",
    "- Num epochs: 5 \n",
    "- Weight decay: 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cae66ccfe7d41e696d8942a3f61b4a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='835' max='835' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [835/835 06:07, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.396439</td>\n",
       "      <td>0.818862</td>\n",
       "      <td>0.781193</td>\n",
       "      <td>0.896266</td>\n",
       "      <td>0.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.410450</td>\n",
       "      <td>0.839820</td>\n",
       "      <td>0.821963</td>\n",
       "      <td>0.854671</td>\n",
       "      <td>0.791667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.276900</td>\n",
       "      <td>0.669830</td>\n",
       "      <td>0.839820</td>\n",
       "      <td>0.810619</td>\n",
       "      <td>0.905138</td>\n",
       "      <td>0.733974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.276900</td>\n",
       "      <td>0.930456</td>\n",
       "      <td>0.841317</td>\n",
       "      <td>0.806569</td>\n",
       "      <td>0.936441</td>\n",
       "      <td>0.708333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.276900</td>\n",
       "      <td>0.840534</td>\n",
       "      <td>0.854790</td>\n",
       "      <td>0.830716</td>\n",
       "      <td>0.911877</td>\n",
       "      <td>0.762821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='42' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [42/42 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Evaluation Results:\n",
      "{'eval_loss': 0.8405343294143677, 'eval_accuracy': 0.8547904191616766, 'eval_f1': 0.8307155322862129, 'eval_precision': 0.9118773946360154, 'eval_recall': 0.7628205128205128, 'eval_runtime': 5.6238, 'eval_samples_per_second': 118.781, 'eval_steps_per_second': 7.468, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "bert_gridsearch_model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_results_best_gridsearch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=5e-05,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=bert_gridsearch_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "results = trainer.evaluate()\n",
    "print(\"BERT Evaluation Results:\")\n",
    "print(results)\n",
    "\n",
    "trainer.save_model(\"./bert_gridsearch_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/VridhiJain/bert_gridsearch_2', endpoint='https://huggingface.co', repo_type='model', repo_id='VridhiJain/bert_gridsearch_2')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "# Repo name\n",
    "repo_name = \"bert_gridsearch_2\"\n",
    "\n",
    "# This creates a repo under your namespace (username)\n",
    "api.create_repo(repo_id=repo_name, private=False, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8f51a4cdc094a16916c4077aee7a45f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b10a5bed044f4dbca5c42c684b6d3b58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/VridhiJain/bert_gridsearch_2/commit/a031f5e97d5dc481bc56190f2def1b6284353af2', commit_message='Upload tokenizer', commit_description='', oid='a031f5e97d5dc481bc56190f2def1b6284353af2', pr_url=None, repo_url=RepoUrl('https://huggingface.co/VridhiJain/bert_gridsearch_2', endpoint='https://huggingface.co', repo_type='model', repo_id='VridhiJain/bert_gridsearch_2'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push model and tokenizer\n",
    "trainer.model.push_to_hub(\"bert_gridsearch_2\")\n",
    "tokenizer.push_to_hub(\"bert_gridsearch_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Optimization (using optuna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "def model_init():\n",
    "  return AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "def objective(trial):\n",
    "  # hyperparameter search space\n",
    "  learning_rate = trial.suggest_float(\"learning_rate\", 1e-6, 5e-5, log=True)\n",
    "  weight_decay = trial.suggest_float(\"weight_decay\", 0.0, 0.3)\n",
    "  batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32])\n",
    "  num_train_epochs = trial.suggest_int(\"num_train_epochs\", 2, 5)\n",
    "\n",
    "  args = TrainingArguments(\n",
    "    output_dir=f\"./bert_bayesian_tuned_lr{learning_rate}_ep{num_train_epochs}_wd{weight_decay}\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    report_to=\"none\"\n",
    "  )\n",
    "\n",
    "  trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "  )\n",
    "\n",
    "  trainer.train()\n",
    "  eval_result = trainer.evaluate()\n",
    "  return eval_result[\"eval_f1\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-07 02:02:19,474] A new study created in memory with name: no-name-cd901520-a5a5-461a-b984-9bcf15373fc6\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='336' max='336' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [336/336 04:26, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.466760</td>\n",
       "      <td>0.784431</td>\n",
       "      <td>0.791304</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.405856</td>\n",
       "      <td>0.821856</td>\n",
       "      <td>0.812006</td>\n",
       "      <td>0.800623</td>\n",
       "      <td>0.823718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.413123</td>\n",
       "      <td>0.829341</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.796407</td>\n",
       "      <td>0.852564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.426895</td>\n",
       "      <td>0.839820</td>\n",
       "      <td>0.821963</td>\n",
       "      <td>0.854671</td>\n",
       "      <td>0.791667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21/21 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-07 02:06:53,172] Trial 0 finished with value: 0.8219633943427621 and parameters: {'learning_rate': 1.8508075746385966e-05, 'weight_decay': 0.09499825421417672, 'batch_size': 32, 'num_train_epochs': 4}. Best is trial 0 with value: 0.8219633943427621.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='668' max='668' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [668/668 02:55, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.601000</td>\n",
       "      <td>0.699102</td>\n",
       "      <td>0.674230</td>\n",
       "      <td>0.681967</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.637400</td>\n",
       "      <td>0.555956</td>\n",
       "      <td>0.718563</td>\n",
       "      <td>0.676976</td>\n",
       "      <td>0.729630</td>\n",
       "      <td>0.631410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='84' max='84' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [84/84 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-07 02:09:56,270] Trial 1 finished with value: 0.6769759450171822 and parameters: {'learning_rate': 4.144451980447425e-06, 'weight_decay': 0.20786572117414442, 'batch_size': 8, 'num_train_epochs': 2}. Best is trial 0 with value: 0.8219633943427621.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1002' max='1002' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1002/1002 04:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.555719</td>\n",
       "      <td>0.738024</td>\n",
       "      <td>0.702886</td>\n",
       "      <td>0.747292</td>\n",
       "      <td>0.663462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.610900</td>\n",
       "      <td>0.476841</td>\n",
       "      <td>0.797904</td>\n",
       "      <td>0.776860</td>\n",
       "      <td>0.802048</td>\n",
       "      <td>0.753205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.465700</td>\n",
       "      <td>0.471277</td>\n",
       "      <td>0.784431</td>\n",
       "      <td>0.754266</td>\n",
       "      <td>0.806569</td>\n",
       "      <td>0.708333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='84' max='84' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [84/84 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-07 02:14:26,713] Trial 2 finished with value: 0.7542662116040956 and parameters: {'learning_rate': 4.951619131331183e-06, 'weight_decay': 0.11474886906471249, 'batch_size': 8, 'num_train_epochs': 3}. Best is trial 0 with value: 0.8219633943427621.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='668' max='668' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [668/668 04:54, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.448176</td>\n",
       "      <td>0.791916</td>\n",
       "      <td>0.736243</td>\n",
       "      <td>0.902326</td>\n",
       "      <td>0.621795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.406413</td>\n",
       "      <td>0.832335</td>\n",
       "      <td>0.820513</td>\n",
       "      <td>0.820513</td>\n",
       "      <td>0.820513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.321300</td>\n",
       "      <td>0.527646</td>\n",
       "      <td>0.848802</td>\n",
       "      <td>0.826758</td>\n",
       "      <td>0.889299</td>\n",
       "      <td>0.772436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.321300</td>\n",
       "      <td>0.662746</td>\n",
       "      <td>0.847305</td>\n",
       "      <td>0.822917</td>\n",
       "      <td>0.897727</td>\n",
       "      <td>0.759615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='42' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [42/42 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-07 02:19:28,859] Trial 3 finished with value: 0.8229166666666666 and parameters: {'learning_rate': 2.322620076809675e-05, 'weight_decay': 0.039382382431990014, 'batch_size': 16, 'num_train_epochs': 4}. Best is trial 3 with value: 0.8229166666666666.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='252' max='252' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [252/252 03:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.681662</td>\n",
       "      <td>0.613772</td>\n",
       "      <td>0.556701</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.519231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.664767</td>\n",
       "      <td>0.672156</td>\n",
       "      <td>0.612389</td>\n",
       "      <td>0.683794</td>\n",
       "      <td>0.554487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.657458</td>\n",
       "      <td>0.684132</td>\n",
       "      <td>0.646566</td>\n",
       "      <td>0.677193</td>\n",
       "      <td>0.618590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21/21 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-07 02:22:56,157] Trial 4 finished with value: 0.6465661641541038 and parameters: {'learning_rate': 1.72483038806336e-06, 'weight_decay': 0.28532904003413123, 'batch_size': 32, 'num_train_epochs': 3}. Best is trial 3 with value: 0.8229166666666666.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='420' max='420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [420/420 05:33, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.428017</td>\n",
       "      <td>0.809880</td>\n",
       "      <td>0.805513</td>\n",
       "      <td>0.771261</td>\n",
       "      <td>0.842949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.378352</td>\n",
       "      <td>0.847305</td>\n",
       "      <td>0.838608</td>\n",
       "      <td>0.828125</td>\n",
       "      <td>0.849359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.411184</td>\n",
       "      <td>0.838323</td>\n",
       "      <td>0.817568</td>\n",
       "      <td>0.864286</td>\n",
       "      <td>0.775641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.516122</td>\n",
       "      <td>0.833832</td>\n",
       "      <td>0.804233</td>\n",
       "      <td>0.894118</td>\n",
       "      <td>0.730769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.540089</td>\n",
       "      <td>0.835329</td>\n",
       "      <td>0.815436</td>\n",
       "      <td>0.855634</td>\n",
       "      <td>0.778846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21/21 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-07 02:28:36,360] Trial 5 finished with value: 0.8154362416107382 and parameters: {'learning_rate': 2.581111550748752e-05, 'weight_decay': 0.18679526297932084, 'batch_size': 32, 'num_train_epochs': 5}. Best is trial 3 with value: 0.8229166666666666.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='335' max='501' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [335/501 02:20 < 01:09, 2.38 it/s, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.527109</td>\n",
       "      <td>0.779940</td>\n",
       "      <td>0.773498</td>\n",
       "      <td>0.744807</td>\n",
       "      <td>0.804487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22/42 00:02 < 00:02, 7.44 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='84' max='84' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [84/84 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-07 02:36:55,880] Trial 7 finished with value: 0.8096885813148789 and parameters: {'learning_rate': 1.335342919991641e-05, 'weight_decay': 0.08200623379037976, 'batch_size': 8, 'num_train_epochs': 3}. Best is trial 3 with value: 0.8229166666666666.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='420' max='420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [420/420 05:32, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.425388</td>\n",
       "      <td>0.811377</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.768786</td>\n",
       "      <td>0.852564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.368836</td>\n",
       "      <td>0.838323</td>\n",
       "      <td>0.828025</td>\n",
       "      <td>0.822785</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.413692</td>\n",
       "      <td>0.841317</td>\n",
       "      <td>0.818493</td>\n",
       "      <td>0.878676</td>\n",
       "      <td>0.766026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.491440</td>\n",
       "      <td>0.841317</td>\n",
       "      <td>0.819728</td>\n",
       "      <td>0.873188</td>\n",
       "      <td>0.772436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.571567</td>\n",
       "      <td>0.830838</td>\n",
       "      <td>0.809444</td>\n",
       "      <td>0.854093</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21/21 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-07 02:42:36,084] Trial 8 finished with value: 0.8094435075885329 and parameters: {'learning_rate': 2.5165929812911236e-05, 'weight_decay': 0.04088012760133073, 'batch_size': 32, 'num_train_epochs': 5}. Best is trial 3 with value: 0.8229166666666666.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='420' max='420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [420/420 05:33, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.611633</td>\n",
       "      <td>0.735030</td>\n",
       "      <td>0.710311</td>\n",
       "      <td>0.725753</td>\n",
       "      <td>0.695513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.533084</td>\n",
       "      <td>0.763473</td>\n",
       "      <td>0.729452</td>\n",
       "      <td>0.783088</td>\n",
       "      <td>0.682692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.498243</td>\n",
       "      <td>0.782934</td>\n",
       "      <td>0.768000</td>\n",
       "      <td>0.766773</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.479208</td>\n",
       "      <td>0.782934</td>\n",
       "      <td>0.761120</td>\n",
       "      <td>0.783051</td>\n",
       "      <td>0.740385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.475053</td>\n",
       "      <td>0.784431</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21/21 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-07 02:48:16,746] Trial 9 finished with value: 0.7647058823529411 and parameters: {'learning_rate': 4.658196705105442e-06, 'weight_decay': 0.055988288110807194, 'batch_size': 32, 'num_train_epochs': 5}. Best is trial 3 with value: 0.8229166666666666.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='643' max='668' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [643/668 04:36 < 00:10, 2.32 it/s, Epoch 3.84/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.419866</td>\n",
       "      <td>0.799401</td>\n",
       "      <td>0.753676</td>\n",
       "      <td>0.883621</td>\n",
       "      <td>0.657051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.456029</td>\n",
       "      <td>0.823353</td>\n",
       "      <td>0.795139</td>\n",
       "      <td>0.867424</td>\n",
       "      <td>0.733974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.278200</td>\n",
       "      <td>0.833826</td>\n",
       "      <td>0.815868</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.927602</td>\n",
       "      <td>0.657051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21/21 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-07 03:07:55,577] Trial 13 finished with value: 0.8178694158075601 and parameters: {'learning_rate': 4.540353487980111e-05, 'weight_decay': 0.10030459209425613, 'batch_size': 32, 'num_train_epochs': 4}. Best is trial 3 with value: 0.8229166666666666.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='334' max='334' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [334/334 02:26, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.408268</td>\n",
       "      <td>0.823353</td>\n",
       "      <td>0.805921</td>\n",
       "      <td>0.827703</td>\n",
       "      <td>0.785256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.406758</td>\n",
       "      <td>0.839820</td>\n",
       "      <td>0.819562</td>\n",
       "      <td>0.864769</td>\n",
       "      <td>0.778846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='42' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [42/42 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-07 03:10:29,675] Trial 14 finished with value: 0.8195615514333895 and parameters: {'learning_rate': 2.1109800177428915e-05, 'weight_decay': 0.163513477011474, 'batch_size': 16, 'num_train_epochs': 2}. Best is trial 3 with value: 0.8229166666666666.\n"
     ]
    }
   ],
   "source": [
    "# run optimization loop\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert-based Classifier retrained w/ best hyperparameters (from Bayesian optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best configuration (best F1):\n",
    "- Learning rate: 4.733978326237281e-05\n",
    "- Weight decay: 0.13720158843680744\n",
    "- Batch size: 8\n",
    "- Num epochs: 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1336' max='1336' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1336/1336 05:49, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.343576</td>\n",
       "      <td>0.851796</td>\n",
       "      <td>0.841091</td>\n",
       "      <td>0.842444</td>\n",
       "      <td>0.839744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.443100</td>\n",
       "      <td>0.377412</td>\n",
       "      <td>0.872754</td>\n",
       "      <td>0.859967</td>\n",
       "      <td>0.884746</td>\n",
       "      <td>0.836538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.173200</td>\n",
       "      <td>0.594932</td>\n",
       "      <td>0.872754</td>\n",
       "      <td>0.859038</td>\n",
       "      <td>0.890034</td>\n",
       "      <td>0.830128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.173200</td>\n",
       "      <td>0.681367</td>\n",
       "      <td>0.869760</td>\n",
       "      <td>0.856198</td>\n",
       "      <td>0.883959</td>\n",
       "      <td>0.830128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='84' max='84' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [84/84 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Evaluation Results:\n",
      "{'eval_loss': 0.6813668608665466, 'eval_accuracy': 0.8697604790419161, 'eval_f1': 0.856198347107438, 'eval_precision': 0.8839590443686007, 'eval_recall': 0.8301282051282052, 'eval_runtime': 6.2328, 'eval_samples_per_second': 107.176, 'eval_steps_per_second': 13.477, 'epoch': 4.0}\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "bert_bayesian_model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_results_best_bayesian\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=4.733978326237281e-05,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.13720158843680744,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=bert_bayesian_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "results = trainer.evaluate()\n",
    "print(\"BERT Evaluation Results:\")\n",
    "print(results)\n",
    "\n",
    "trainer.save_model(\"./bert_bayesian_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/VridhiJain/bert_bayesian_2', endpoint='https://huggingface.co', repo_type='model', repo_id='VridhiJain/bert_bayesian_2')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Repo name\n",
    "repo_name = \"bert_bayesian_2\"\n",
    "\n",
    "# This creates a repo under your namespace (username)\n",
    "api.create_repo(repo_id=repo_name, private=False, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "955035ff8634433bb8732c1e9db9b38a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a69adf9cf5804eb2b8ae7ceef4ecc18b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "861aa402437042428266d25f0dd57794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/VridhiJain/bert_bayesian_2/commit/d04495efa7eb2c2a182716028d0799915c9a70ae', commit_message='Upload tokenizer', commit_description='', oid='d04495efa7eb2c2a182716028d0799915c9a70ae', pr_url=None, repo_url=RepoUrl('https://huggingface.co/VridhiJain/bert_bayesian_2', endpoint='https://huggingface.co', repo_type='model', repo_id='VridhiJain/bert_bayesian_2'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push to your Hugging Face repo\n",
    "trainer.push_to_hub(\"bert_bayesian_2\")\n",
    "tokenizer.push_to_hub(\"bert_bayesian_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa-based Classifier - Hyperparameter Tuning (don't rerun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf023d1b0ec741eca8a29a6641e32146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b96ba03fb9554e9e84ad81d30b6e5022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "\n",
    "tokenizer_roberta = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "train_dataset = Dataset.from_pandas(train_df[[\"headline\", \"label\"]])\n",
    "test_dataset = Dataset.from_pandas(test_df[[\"headline\", \"label\"]])\n",
    "\n",
    "def tokenize_roberta(batch):\n",
    "    return tokenizer_roberta(batch[\"headline\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_roberta, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_roberta, batched=True)\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# define search space\n",
    "learning_rates = [1e-5, 2e-5, 3e-5, 5e-5]\n",
    "epochs = [3, 5]\n",
    "weight_decays = [0.01, 0.001]\n",
    "\n",
    "best_f1 = 0\n",
    "best_config = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "  for num_epochs in epochs:\n",
    "    for wd in weight_decays:\n",
    "      print(f\"\\nTraining with lr={lr}, epochs={num_epochs}, weight_decay={wd}\")\n",
    "\n",
    "      training_args = TrainingArguments(\n",
    "        output_dir=f\"./roberta_tuned_lr{lr}_ep{num_epochs}_wd{wd}\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=num_epochs,\n",
    "        weight_decay=wd,\n",
    "        report_to=\"none\",\n",
    "        logging_strategy=\"epoch\"\n",
    "      )\n",
    "\n",
    "      model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "\n",
    "      trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        compute_metrics=compute_metrics\n",
    "      )\n",
    "\n",
    "      trainer.train()\n",
    "      eval_results = trainer.evaluate()\n",
    "      print(\"F1 score:\", eval_results['eval_f1'])\n",
    "\n",
    "      if eval_results['eval_f1'] > best_f1:\n",
    "        best_f1 = eval_results['eval_f1']\n",
    "        best_config = {\n",
    "            \"learning_rate\": lr,\n",
    "            \"num_epochs\": num_epochs,\n",
    "            \"weight_decay\": wd,\n",
    "            \"eval_results\": eval_results\n",
    "        }\n",
    "\n",
    "print(\"\\nBest configuration:\")\n",
    "print(best_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa-based Classifier retrained w/ best hyperparameters (from grid search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best configuration (best F1):\n",
    "- Learning rate: 2e-05\n",
    "- Weight decay: 0.001\n",
    "- Num epochs: 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_gridsearch_model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "\n",
    "training_args_roberta = TrainingArguments(\n",
    "    output_dir=\"./roberta_gridsearch_results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    learning_rate=2e-05,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.001,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer_roberta = Trainer(\n",
    "    model=roberta_gridsearch_model,\n",
    "    args=training_args_roberta,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer_roberta.train()\n",
    "roberta_results = trainer_roberta.evaluate()\n",
    "print(\"RoBERTa Evaluation Results:\", roberta_results)\n",
    "\n",
    "trainer_roberta.save_model(\"./roberta_gridsearch_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa - Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "  return RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "\n",
    "def objective(trial):\n",
    "  # hyperparameter search space\n",
    "  learning_rate = trial.suggest_float(\"learning_rate\", 1e-6, 7e-5, log=True)\n",
    "  weight_decay = trial.suggest_float(\"weight_decay\", 0.0, 0.05)\n",
    "  batch_size = trial.suggest_categorical(\"batch_size\", [8, 16])\n",
    "  num_train_epochs = trial.suggest_int(\"num_train_epochs\", 3, 6)\n",
    "\n",
    "  args = TrainingArguments(\n",
    "    output_dir=f\"./roberta_bayesian_tuned_lr{learning_rate}_ep{num_train_epochs}_wd{weight_decay}\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    report_to=\"none\"\n",
    "  )\n",
    "\n",
    "  trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "  )\n",
    "\n",
    "  trainer.train()\n",
    "  eval_result = trainer.evaluate()\n",
    "  return eval_result[\"eval_f1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run optimization loop\n",
    "study_roberta = optuna.create_study(direction=\"maximize\")\n",
    "study_roberta.optimize(objective, n_trials=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
